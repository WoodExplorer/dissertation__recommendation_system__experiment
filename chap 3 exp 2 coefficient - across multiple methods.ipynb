{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "import utility_synopsis\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "#from surprise import BaselineOnly\n",
    "#from surprise import Dataset\n",
    "#from surprise import evaluate\n",
    "from surprise import Reader\n",
    "import collections as coll\n",
    "\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_limit = 1000 # only calculate neighbors of a subset of users to avoid space and time problems\n",
    "\n",
    "def extract_item_info(filename, delimiter, genre_delimiter):\n",
    "    data = {}\n",
    "\n",
    "    with open(filename , 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            itemId, title, genre_list = map(lambda x: x.strip(), line.split(delimiter))\n",
    "            \n",
    "            data[itemId] = (title, genre_list.split(genre_delimiter))\n",
    "    return data\n",
    "\n",
    "item_file_name, item_file_delimiter, genre_delimiter = os.path.sep.join(['ml-1m', 'movies.dat']), '::', '|'\n",
    "item_info = extract_item_info(item_file_name, item_file_delimiter, genre_delimiter)\n",
    "\n",
    "###\n",
    "def extract_user_item_interaction(filename, delimiter):\n",
    "    data = {}\n",
    "\n",
    "    with open(filename , 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            userId, movieId, rating, timestamp = line.split(delimiter)\n",
    "            #userId = int(userId)\n",
    "            #movieId = int(movieId)\n",
    "            rating = float(rating)\n",
    "            timestamp = int(timestamp)\n",
    "\n",
    "            if userId not in data:\n",
    "                data[userId] = []\n",
    "            data[userId].append((movieId, rating, timestamp))\n",
    "    \n",
    "    # order by time\n",
    "    for userId in data:\n",
    "        data[userId].sort(key=lambda x: x[2]) \n",
    "    return data\n",
    "\n",
    "rating_file_name, rating_file_delimiter = os.path.sep.join(['ml-1m', 'ratings.dat']), '::'\n",
    "user_item_interaction = extract_user_item_interaction(rating_file_name, rating_file_delimiter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:45:46,807 : INFO : loading Word2Vec object from /home/wsyj/dissertation__recommendation_system__experiment_2/dissertation__recommendation_system__experiment/main_modelnum_features=100_min_count=1_window=1_iter=30.model\n",
      "2017-02-24 19:45:46,816 : INFO : loading wv recursively from /home/wsyj/dissertation__recommendation_system__experiment_2/dissertation__recommendation_system__experiment/main_modelnum_features=100_min_count=1_window=1_iter=30.model.wv.* with mmap=None\n",
      "2017-02-24 19:45:46,816 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-02-24 19:45:46,817 : INFO : setting ignored attribute cum_table to None\n",
      "2017-02-24 19:45:46,817 : INFO : loaded /home/wsyj/dissertation__recommendation_system__experiment_2/dissertation__recommendation_system__experiment/main_modelnum_features=100_min_count=1_window=1_iter=30.model\n"
     ]
    }
   ],
   "source": [
    "# preparation for word2vec approach\n",
    "\n",
    "model_path = '/home/wsyj/dissertation__recommendation_system__experiment_2/dissertation__recommendation_system__experiment/main_modelnum_features=100_min_count=1_window=1_iter=30.model'\n",
    "model = gensim.models.Word2Vec.load(model_path)\n",
    "\n",
    "def user_history2user_repr__simple_average(model, target_user_history): # target_user_history: It should_be_a_list_of_tuples_included_items.\n",
    "    #print 'target_user_history:', target_user_history\n",
    "    items_existed_in_model = filter(lambda x: x[0] in model, target_user_history)\n",
    "    #print 'items_existed_in_model:', items_existed_in_model[0]\n",
    "    items_translated_to_vecs = map(lambda x: model[x[0]], items_existed_in_model)\n",
    "    #print 'items_translated_to_vecs:', items_translated_to_vecs[0]\n",
    "    return np.average(items_translated_to_vecs, axis=0)   \n",
    "\n",
    "#user_history2user_repr__simple_average(model, user_item_interaction['5989'])\n",
    "# calculate user representation dict\n",
    "user_repr__word2vec = {user: user_history2user_repr__simple_average(model, user_item_interaction[user]) for user in user_item_interaction}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### calculate neighbors of each user\n",
    "neighbors__word2vec = {}\n",
    "\n",
    "for i, target_user in enumerate(user_item_interaction):\n",
    "    if i == count_limit:\n",
    "        break\n",
    "        \n",
    "    target_repr = user_repr__word2vec[target_user]\n",
    "    #print target_repr\n",
    "    simi = [(v, target_repr.dot(user_repr__word2vec[v]) / (la.norm(target_repr * la.norm(user_repr__word2vec[v]))))\n",
    "            for v in user_item_interaction]\n",
    "    simi.sort(key=lambda x: -1 * x[1])\n",
    "    \n",
    "    #print simi[1:K + 1]\n",
    "    #print simi[-1 * K:]\n",
    "    neighbors__word2vec[target_user] = simi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('chap3_exp2__word2vec__user_repr.dump', 'wb') as fout:\n",
    "    pickle.dump(user_repr__word2vec, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e66dfece2070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chap3_exp2__word2vec__neighbors.dump'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbors__word2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This might take a long time\n",
    "\n",
    "with open('chap3_exp2__word2vec__neighbors.dump', 'wb') as fout:\n",
    "    pickle.dump(neighbors__word2vec, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preparation for simple approach/CF approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preparation for content-based approach\n",
    "# preparation for calculation of nearest neighbors of a target in content-based approach\n",
    "genres = [\"Action\",\n",
    "\"Adventure\",\n",
    "\"Animation\",\n",
    "\"Children's\",\n",
    "\"Comedy\",\n",
    "\"Crime\",\n",
    "\"Documentary\",\n",
    "\"Drama\",\n",
    "\"Fantasy\",\n",
    "\"Film-Noir\",\n",
    "\"Horror\",\n",
    "\"Musical\",\n",
    "\"Mystery\",\n",
    "\"Romance\",\n",
    "\"Sci-Fi\",\n",
    "\"Thriller\",\n",
    "\"War\",\n",
    "\"Western\",]\n",
    "\n",
    "genres_index_dict = dict(zip(*[genres, range(len(genres))]))\n",
    "index_genres_dict = dict(zip(*[range(len(genres)), genres]))\n",
    "\n",
    "def extract_genres(filename, delimiter, genre_delimiter):\n",
    "    data = {}\n",
    "\n",
    "    with open(filename , 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            itemId, title, genre_list = map(lambda x: x.strip(), line.split(delimiter))\n",
    "            \n",
    "            data[itemId] = genre_list.split(genre_delimiter)\n",
    "    return data\n",
    "\n",
    "item_file_name, item_file_delimiter, genre_delimiter = os.path.sep.join(['ml-1m', 'movies.dat']), '::', '|'\n",
    "item_info = extract_genres(item_file_name, item_file_delimiter, genre_delimiter)\n",
    "\n",
    "def tmp_set(vec, i, val):\n",
    "    vec[i] = val\n",
    "    \n",
    "def generate_item_repr(item_info):\n",
    "    item_repr = {}\n",
    "    for item in item_info:\n",
    "        #print item\n",
    "        f = np.array([0] * len(genres))\n",
    "        map(lambda x: tmp_set(f, genres_index_dict[x], 1), item_info[item])\n",
    "        #print f\n",
    "        item_repr[item] = f\n",
    "        #break\n",
    "    return item_repr\n",
    "\n",
    "cb_item_repr = generate_item_repr(item_info)\n",
    "\n",
    "###\n",
    "def user_history2user_repr__simple_average(cb_item_repr, target_user_history): # target_user_history: It should_be_a_list_of_tuples_included_items.\n",
    "    items_translated_to_vecs = map(lambda x: cb_item_repr[x[0]], target_user_history)\n",
    "    #print 'items_translated_to_vecs:', items_translated_to_vecs[0]\n",
    "    return np.average(items_translated_to_vecs, axis=0)   \n",
    "\n",
    "# calculate user representation dict\n",
    "user_repr__cb = {user: user_history2user_repr__simple_average(cb_item_repr, user_item_interaction[user]) for user in user_item_interaction}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.42857143,  0.19480519,  0.01298701,  0.03246753,  0.22077922,\n",
       "        0.03246753,  0.        ,  0.18181818,  0.05194805,  0.01948052,\n",
       "        0.15584416,  0.00649351,  0.02597403,  0.04545455,  0.68181818,\n",
       "        0.23376623,  0.05844156,  0.01948052])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_repr__cb['5389']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info of train:\n",
      "6040\n",
      "3706\n",
      "1000209\n",
      "info of test:\n",
      "0\n",
      "[Note]: SVD dump does not exist. Going to train a SVD!\n",
      "svd time consumption: 45\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'inner_iid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-42e6bfda553b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0muser_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0msvd_user_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_svd_user_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_item_interaction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m### calculate neighbors of each user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-42e6bfda553b>\u001b[0m in \u001b[0;36mgenerate_svd_user_repr\u001b[0;34m(user_item_interaction)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0minner_uid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_inner_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0muser_repr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minner_iid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0msvd_missing_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'inner_iid' is not defined"
     ]
    }
   ],
   "source": [
    "# preparation for svd approach\n",
    "data_filename, delimiter, data_set = os.path.sep.join(['ml-1m', 'ratings.dat']), '::', '1M'\n",
    "\n",
    "chap3_exp2_train_file_name = 'chap3_exp2__tmp__svd_train'\n",
    "chap3_exp2_test_file_name = 'chap3_exp2__tmp__svd_test'\n",
    "my_sep = ':'\n",
    "\n",
    "if not os.path.exists(chap3_exp2__svd_dump):\n",
    "    with open(chap3_exp2_test_file_name, 'w') as f: # Yes, emtpy test file. We are going to use full dataset.\n",
    "        pass\n",
    "    with open(chap3_exp2_train_file_name, 'w') as fout: # Write full dataset into train file.\n",
    "        with open(data_filename, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                fout.write(my_sep.join(line.split(delimiter)))\n",
    "#\n",
    "\n",
    "##\n",
    "reader = Reader(line_format='user item rating timestamp', sep=my_sep)\n",
    "\n",
    "data = Dataset.load_from_folds([(chap3_exp2_train_file_name, chap3_exp2_test_file_name)], reader=reader)\n",
    "\n",
    "for trainset, testset in data.folds():\n",
    "    first_train, first_test = trainset, testset\n",
    "    \n",
    "print 'info of train:'\n",
    "print first_train.n_users\n",
    "print first_train.n_items\n",
    "print first_train.n_ratings\n",
    "\n",
    "print 'info of test:'\n",
    "print len(first_test)\n",
    "\n",
    "\n",
    "##\n",
    "algo = None\n",
    "chap3_exp2__svd_dump = 'chap3_exp2__svd_dump.algo'\n",
    "if os.path.exists(chap3_exp2__svd_dump):\n",
    "    print '[Note]: SVD dump exists already. Going to load it!'\n",
    "    with open(chap3_exp2__svd_dump, 'rb') as f:\n",
    "        algo = pickle.load(f)\n",
    "else:    \n",
    "    print '[Note]: SVD dump does not exist. Going to train a SVD!'\n",
    "    # train SVD\n",
    "    starttime = datetime.datetime.now()\n",
    "\n",
    "    algo = SVD()\n",
    "    algo.train(first_train)\n",
    "\n",
    "    ###\n",
    "    endtime = datetime.datetime.now()\n",
    "    interval = (endtime - starttime).seconds\n",
    "    print 'svd time consumption: %d' % (interval)\n",
    "\n",
    "\n",
    "##\n",
    "#all_user_ids_in_svd_train = set()\n",
    "#with open(train_file_name, 'r') as fin:\n",
    "#    for i, line in enumerate(fin):\n",
    "#        userId, movieId, rating, timestamp = line.split(my_sep)\n",
    "#        all_user_ids_in_svd_train.add(userId)\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svd_missing_count: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##\n",
    "def generate_svd_user_repr(user_item_interaction):\n",
    "    '''Constraint for @para: There should be some method to extract all user ids from the @para.'''\n",
    "    user_repr = {}\n",
    "    svd_missing_count = 0\n",
    "    for user in user_item_interaction:\n",
    "        \n",
    "        try:\n",
    "            inner_uid = first_train.to_inner_uid(user)\n",
    "            user_repr[user] = algo.pu[inner_uid]\n",
    "        except ValueError, e:\n",
    "            svd_missing_count += 1\n",
    "    print 'svd_missing_count:', svd_missing_count\n",
    "    return user_repr\n",
    "\n",
    "svd_user_repr = generate_svd_user_repr(user_item_interaction)\n",
    "\n",
    "### calculate neighbors of each user\n",
    "neighbors__svd = {}\n",
    "\n",
    "for i, target_user in enumerate(user_item_interaction):\n",
    "    if i == count_limit:\n",
    "        break\n",
    "        \n",
    "    target_repr = svd_user_repr[target_user]\n",
    "    #print target_repr\n",
    "    simi = [(v, target_repr.dot(svd_user_repr[v]) / (la.norm(target_repr * la.norm(svd_user_repr[v]))))\n",
    "            for v in user_item_interaction]\n",
    "    simi.sort(key=lambda x: -1 * x[1])\n",
    "    \n",
    "    #print simi[1:K + 1]\n",
    "    #print simi[-1 * K:]\n",
    "    neighbors__svd[target_user] = simi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5989'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_target_users[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169,\n",
       "        0.07320169,  0.07320169,  0.07320169,  0.07320169,  0.07320169])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_user_repr['5988']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196,\n",
       "        0.04600196,  0.04600196,  0.04600196,  0.04600196,  0.04600196])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_user_repr['5989']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_user_repr['5989'].dot(svd_user_repr['5988']) / (la.norm(svd_user_repr['5989'] * la.norm(svd_user_repr['5988'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527,\n",
       "        0.02720527,  0.02720527,  0.02720527,  0.02720527,  0.02720527])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.pu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943,\n",
       "        0.04645943,  0.04645943,  0.04645943,  0.04645943,  0.04645943])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.pu[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395,\n",
       "        0.10212395,  0.10212395,  0.10212395,  0.10212395,  0.10212395])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.pu[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_cosine_similarity_with_regression(target, b):\n",
    "    '''ATTENTION: This function might have problem regarding to scale: While value of cosine similarity \n",
    "    ranges from 0(included) to 1(included), the Euclidean distance might not so.'''\n",
    "    cos = target.dot(b) / (la.norm(target * la.norm(b)))\n",
    "    if cos > 0.99999 or cos < -0.99999:\n",
    "        # Euclidean distance\n",
    "        return -1 * abs(target[0] - b[0])  # for consistency: bigger means more similar\n",
    "    else:\n",
    "        return cos # bigger means more similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_target_users = []\n",
    "for i, target_user in enumerate(user_item_interaction):\n",
    "    if i == count_limit:\n",
    "        break\n",
    "    all_target_users.append(target_user)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0/1000\n",
      "neighbors__svd: [('5988', -0.0), ('4090', -1.4184216790821824e-05), ('3074', -2.5446335330497893e-05), ('4753', -2.8442956761937088e-05), ('2275', -3.2858889629949761e-05), ('83', -7.2446903137277086e-05), ('466', -9.5574485427143974e-05), ('2416', -9.7966415514808003e-05), ('2014', -0.00011041765816349747), ('4966', -0.00012157335688735282)] ... [('3151', -0.16927314766211182), ('1198', -0.17496766308106282), ('1790', -0.1794772894462951), ('3537', -0.18701478435385538), ('2744', -0.18749106030001611), ('2458', -0.19245506332582168), ('2195', -0.19469324582868119), ('3679', -0.19985060387038639), ('4539', -0.2189815402125761), ('3610', -0.26431945941847512)]\n",
      "neighbors__word2vec: [('5988', 1.0), ('5718', 0.9841646), ('2174', 0.98250049), ('1532', 0.98213619), ('3240', 0.98212159), ('4452', 0.98195195), ('5998', 0.98069644), ('11', 0.98042107), ('3057', 0.98027587), ('111', 0.98024207)] ... [('4211', 0.80288029), ('5999', 0.79902619), ('3623', 0.79784542), ('5069', 0.79479998), ('304', 0.7904743), ('2053', 0.7885316), ('1226', 0.78781724), ('94', 0.78468627), ('986', 0.77609187), ('5772', 0.76811224)]\n",
      "77\n",
      "1523\n",
      "\n",
      "\n",
      "neighbors__cb: [('5988', 1.0), ('5153', 0.99455293819822932), ('2233', 0.99445811485333724), ('3572', 0.99306420099282489), ('952', 0.99298263666386155), ('1273', 0.9924496274828174), ('2561', 0.99202634140652546), ('2222', 0.99196714022634436), ('2951', 0.99196422333991707), ('4389', 0.99137830227765211)] ... [('46', 0.20054014926672276), ('2908', 0.1940941074769687), ('1357', 0.1894726876668465), ('2204', 0.1867007198101881), ('5929', 0.18561196097474703), ('3806', 0.18353606029466607), ('278', 0.18116172535762393), ('3528', 0.17306962312460819), ('3801', 0.16841430922144823), ('2352', 0.15708747235234585)]\n",
      "neighbors__word2vec: [('5988', 1.0), ('5718', 0.9841646), ('2174', 0.98250049), ('1532', 0.98213619), ('3240', 0.98212159), ('4452', 0.98195195), ('5998', 0.98069644), ('11', 0.98042107), ('3057', 0.98027587), ('111', 0.98024207)] ... [('4211', 0.80288029), ('5999', 0.79902619), ('3623', 0.79784542), ('5069', 0.79479998), ('304', 0.7904743), ('2053', 0.7885316), ('1226', 0.78781724), ('94', 0.78468627), ('986', 0.77609187), ('5772', 0.76811224)]\n",
      "99\n",
      "701\n",
      "missing_count__cb: 0\n",
      "missing_count__svd: 0\n",
      "(defaultdict(<function <lambda> at 0xa89f8bc4>, {}), defaultdict(<function <lambda> at 0xa89f8f44>, {}))\n"
     ]
    }
   ],
   "source": [
    "# For user, analyze any two neighborhood calculated by simple approach, content-based approach, \n",
    "# word2vec approach and svd approach.\n",
    "\n",
    "\n",
    "def find_out_intersection_size_sum_with_given_limit_list__x(limit_list, all_target_users):\n",
    "    intersection_size_map = coll.defaultdict(lambda:coll.defaultdict(list))\n",
    "    union_size_map = coll.defaultdict(lambda:coll.defaultdict(list))\n",
    "\n",
    "    missing_count__cb = 0\n",
    "    missing_count__svd = 0\n",
    "\n",
    "    i = None\n",
    "    for __step, target_user in enumerate(all_target_users):\n",
    "        #print item_id\n",
    "        if 0 == __step % 100:\n",
    "            print 'progress: %d/%d' % (__step, len(all_target_users))\n",
    "\n",
    "        # content-based <START>\n",
    "        #cb_target_repr = item_repr[item_id] # cb: content based\n",
    "        #sims_cb = [(iid, cb_target_repr.dot(item_repr[iid]) / \n",
    "        #            (la.norm(cb_target_repr * la.norm(item_repr[iid])))) for iid in model.wv.vocab]\n",
    "        ##print sims[0]\n",
    "        #sims_cb = sorted(sims_cb, key=lambda item: -1 * item[1])\n",
    "        target_repr__cb = user_repr__cb[target_user]\n",
    "        simi__cb = [(v, target_repr__cb.dot(user_repr__cb[v]) \n",
    "                 / (la.norm(target_repr__cb * la.norm(user_repr__cb[v]))))\n",
    "                for v in user_item_interaction]\n",
    "        simi__cb.sort(key=lambda x: -1 * x[1])\n",
    "        neighbors__cb = simi__cb\n",
    "        # content-based <END>\n",
    "       \n",
    "    \n",
    "        # svd <START>\n",
    "        #        svd_target_repr = svd_item_repr[item_id]\n",
    "        #        #print 'svd_target_repr:', svd_target_repr\n",
    "        #        sims_svd = [(iid, svd_target_repr.dot(svd_item_repr[iid]) / \n",
    "        #                     (la.norm(svd_target_repr * la.norm(svd_item_repr[iid])))) for iid in all_item_ids_in_svd_train]\n",
    "        #        sims_svd = sorted(sims_svd, key=lambda item: -1 * item[1])\n",
    "        #        #print 'sims_svd:', sims_svd\n",
    "                \n",
    "        target_repr__svd = svd_user_repr[target_user]\n",
    "        simi__svd = [(v, my_cosine_similarity_with_regression(target_repr__svd, svd_user_repr[v]))\n",
    "                for v in user_item_interaction]\n",
    "        simi__svd.sort(key=lambda x: -1 * x[1])\n",
    "        neighbors__svd = simi__svd\n",
    "        ##\n",
    "        # svd <END>\n",
    "\n",
    "        # word2vec <START>\n",
    "        #        ret_by_word2vec = model.most_similar(str(item_id), topn=max(limit_list))\n",
    "        \n",
    "        target_repr__word2vec = user_repr__word2vec[target_user]\n",
    "        simi__word2vec = [(v, target_repr__word2vec.dot(user_repr__word2vec[v]) \n",
    "                 / (la.norm(target_repr__word2vec * la.norm(user_repr__word2vec[v]))))\n",
    "                for v in user_item_interaction]\n",
    "        simi__word2vec.sort(key=lambda x: -1 * x[1])\n",
    "        neighbors__word2vec = simi__word2vec\n",
    "        # word2vec <END>\n",
    "        \n",
    "        print 'neighbors__svd:', neighbors__svd[:10], '...', neighbors__svd[-10:]\n",
    "        print 'neighbors__word2vec:', neighbors__word2vec[:10], '...', neighbors__word2vec[-10:]\n",
    "        \n",
    "        tmp__limit = 800\n",
    "        tmp__svd = [x[0] for x in neighbors__svd[:tmp__limit]]\n",
    "        tmp__word2vec = [x[0] for x in neighbors__word2vec[:tmp__limit]]\n",
    "        print len(set(tmp__svd).intersection(tmp__word2vec))\n",
    "        print len(set(tmp__svd).union(tmp__word2vec))\n",
    "        \n",
    "        print\n",
    "        print\n",
    "        ##\n",
    "        print 'neighbors__cb:', neighbors__cb[:10], '...', neighbors__cb[-10:]\n",
    "        print 'neighbors__word2vec:', neighbors__word2vec[:10], '...', neighbors__word2vec[-10:]\n",
    "        \n",
    "        tmp__limit = 400\n",
    "        tmp__cb = [x[0] for x in neighbors__cb[:tmp__limit]]\n",
    "        tmp__word2vec = [x[0] for x in neighbors__word2vec[:tmp__limit]]\n",
    "        print len(set(tmp__cb).intersection(tmp__word2vec))\n",
    "        print len(set(tmp__cb).union(tmp__word2vec))\n",
    "        break\n",
    "        \n",
    "        for limit in limit_list:\n",
    "            #print(sims[1:limit+1])\n",
    "            #most_similar_by_cb = [x[0] for x in sims_cb[1:limit+1]]  # Careful! The index range!\n",
    "            #print most_similar_by_cb\n",
    "\n",
    "            most_similar_by_svd = [x[0] for x in sims_svd[1:limit+1]]  # Careful! The index range!\n",
    "            \n",
    "            most_similar_by_word2vec = [x[0] for x in ret_by_word2vec][:limit]  # Careful! The index range! Most similar items by word2vec don't include target item itself.\n",
    "            #print most_similar_by_word2vec\n",
    "\n",
    "            \n",
    "            #print 'cb:', most_similar_by_cb\n",
    "            #print 'word2vec:', most_similar_by_word2vec\n",
    "            #print 'svd:', most_similar_by_svd\n",
    "            \n",
    "            #print set(most_similar_by_cb).intersection(set(most_similar_by_word2vec))\n",
    "            inter = set(most_similar_by_cb).intersection(set(most_similar_by_word2vec))\n",
    "            intersection_size_map[limit]['cb__word2vec'].append(len(inter))\n",
    "            union = set(most_similar_by_cb).union(set(most_similar_by_word2vec))\n",
    "            union_size_map[limit]['cb__word2vec'].append(len(union))\n",
    "            \n",
    "            #\n",
    "            inter = set(most_similar_by_cb).intersection(set(most_similar_by_svd))\n",
    "            #                              (len(set(most_similar_by_word2vec).intersection(set(most_similar_by_svd))))\n",
    "            intersection_size_map[limit]['cb__svd'].append(len(inter))\n",
    "            union = set(most_similar_by_cb).union(set(most_similar_by_svd))\n",
    "            union_size_map[limit]['cb__svd'].append(len(union))\n",
    "            \n",
    "            inter = set(most_similar_by_word2vec).intersection(set(most_similar_by_svd))\n",
    "            intersection_size_map[limit]['word2vec__svd'].append(len(inter))\n",
    "            union = set(most_similar_by_word2vec).union(set(most_similar_by_svd))\n",
    "            union_size_map[limit]['word2vec__svd'].append(len(union))\n",
    "                          \n",
    "        #if 0 < len(inter):\n",
    "        #    print item_id\n",
    "        if 0 == __step:\n",
    "            break\n",
    "\n",
    "    print 'missing_count__cb:', missing_count__cb\n",
    "    print 'missing_count__svd:', missing_count__svd\n",
    "\n",
    "    return intersection_size_map, union_size_map\n",
    "\n",
    "r = find_out_intersection_size_sum_with_given_limit_list__x([500], all_target_users)\n",
    "#r = find_out_intersection_size_sum_with_given_limit_list__x(range(10, 500, 10))\n",
    "print r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
