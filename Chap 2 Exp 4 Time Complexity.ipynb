{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim#from gensim.models import word2vec\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(set([1,2,3,4,5]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(lambda x: x % 2 == 1, [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_data_from_file_and_generate_train_set_with_specified_M_and_R(filename, num_of_movies, num_of_rates, delimiter):\n",
    "    train = None\n",
    "    data = []\n",
    "    movie_id = set()\n",
    "\n",
    "    with open(filename , 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            userId, movieId, rating, timestamp = line.split(delimiter)\n",
    "            #userId = int(userId)\n",
    "            #movieId = int(movieId)\n",
    "            rating = float(rating)\n",
    "            timestamp = int(timestamp)\n",
    "            \n",
    "            movie_id.add(movieId)\n",
    "\n",
    "            data.append((userId, movieId, rating, timestamp))\n",
    "\n",
    "    # control number of movies\n",
    "    movies_selected = random.sample(movie_id, num_of_movies)\n",
    "    print 'num_of_movies:'\n",
    "    print 'expected len: %d' % (num_of_movies)\n",
    "    print 'in fact: %d' % len(movies_selected)\n",
    "    \n",
    "    # filter out history records whose movies are not selected\n",
    "    data = filter(lambda x: x[1] in movies_selected, data) # Careful! Hard-coded number!\n",
    "    \n",
    "    # control number of rates\n",
    "    data = random.sample(data, num_of_rates)\n",
    "    print 'num_of_rates:'\n",
    "    print 'expected len: %d' % (num_of_rates)\n",
    "    print 'in fact: %d' % len(data)\n",
    "    \n",
    "    train = {}\n",
    "    for i, t in enumerate(data):\n",
    "        userId, movieId, rating, timestamp = t\n",
    "        if userId not in train:\n",
    "            train[userId] = []\n",
    "        train[userId].append((movieId, rating, timestamp))\n",
    "    \n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_lines: 1000209\n"
     ]
    }
   ],
   "source": [
    "#data_filename, delimiter, data_set = os.path.sep.join(['ml-100k', 'u.data']), '\\t', '100K'\n",
    "data_filename, delimiter, data_set = os.path.sep.join(['ml-1m', 'ratings.dat']), '::', '1M'\n",
    "#data_filename, delimiter, data_set = os.path.sep.join(['ml-10M100K', 'ratings.dat']), '::', '10M'\n",
    "\n",
    "\n",
    "def get_lines(filename):\n",
    "    cnt = 0\n",
    "    with open(filename , 'r') as f:\n",
    "        for cnt, line in enumerate(f):\n",
    "            pass\n",
    "    return cnt + 1\n",
    "\n",
    "total_lines = get_lines(data_filename)\n",
    "print 'total_lines:', total_lines\n",
    "\n",
    "K = None\n",
    "N = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_movies:\n",
      "expected len: 1000\n",
      "in fact: 1000\n",
      "num_of_rates:\n",
      "expected len: 100000\n",
      "in fact: 100000\n"
     ]
    }
   ],
   "source": [
    "num_of_movies = 1000\n",
    "num_of_rates = 10 * 10000\n",
    "r = extract_data_from_file_and_generate_train_set_with_specified_M_and_R(data_filename, num_of_movies, num_of_rates, delimiter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3060', 3.0, 959974376),\n",
       " ('1120', 3.0, 956875950),\n",
       " ('1747', 4.0, 959974407),\n",
       " ('296', 2.0, 956875720),\n",
       " ('3108', 4.0, 959974264),\n",
       " ('2124', 3.0, 959974504),\n",
       " ('1213', 3.0, 956875635),\n",
       " ('588', 3.0, 959974349),\n",
       " ('3253', 3.0, 959974328),\n",
       " ('1265', 4.0, 959974264),\n",
       " ('1569', 2.0, 959974560),\n",
       " ('3450', 3.0, 959974504)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-350337b3b453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print [x for x in l]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "l = [[1,2], [3, 4]]\n",
    "#print [x for x in l]\n",
    "x = None\n",
    "print [y for y in x for x in l]\n",
    "print [y for x in l for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('3060', 3.0, 959974376),\n",
       "  ('1120', 3.0, 956875950),\n",
       "  ('1747', 4.0, 959974407),\n",
       "  ('296', 2.0, 956875720),\n",
       "  ('3108', 4.0, 959974264),\n",
       "  ('2124', 3.0, 959974504),\n",
       "  ('1213', 3.0, 956875635),\n",
       "  ('588', 3.0, 959974349),\n",
       "  ('3253', 3.0, 959974328),\n",
       "  ('1265', 4.0, 959974264),\n",
       "  ('1569', 2.0, 959974560),\n",
       "  ('3450', 3.0, 959974504)],\n",
       " [('2829', 1.0, 957756760),\n",
       "  ('317', 4.0, 956874389),\n",
       "  ('1641', 3.0, 956873721),\n",
       "  ('3053', 5.0, 957756905),\n",
       "  ('2840', 4.0, 957756905),\n",
       "  ('3253', 3.0, 956873744),\n",
       "  ('3512', 5.0, 956873163)]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in r.values()[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3060', 3.0, 959974376),\n",
       " ('1120', 3.0, 956875950),\n",
       " ('1747', 4.0, 959974407),\n",
       " ('296', 2.0, 956875720),\n",
       " ('3108', 4.0, 959974264),\n",
       " ('2124', 3.0, 959974504),\n",
       " ('1213', 3.0, 956875635),\n",
       " ('588', 3.0, 959974349),\n",
       " ('3253', 3.0, 959974328),\n",
       " ('1265', 4.0, 959974264),\n",
       " ('1569', 2.0, 959974560),\n",
       " ('3450', 3.0, 959974504),\n",
       " ('2829', 1.0, 957756760),\n",
       " ('317', 4.0, 956874389),\n",
       " ('1641', 3.0, 956873721),\n",
       " ('3053', 5.0, 957756905),\n",
       " ('2840', 4.0, 957756905),\n",
       " ('3253', 3.0, 956873744),\n",
       " ('3512', 5.0, 956873163)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[y for y in x for x in r.values()[:2]] # WRONG!\n",
    "[y for x in r.values()[:2] for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963\n"
     ]
    }
   ],
   "source": [
    "all_tuples = [y for x in r.values() for y in x]\n",
    "distinct_items = set([x[0] for x in all_tuples])\n",
    "print len(distinct_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3922', '3923', '1718', '3925', '3928', '1869', '1147', '3259', '1948', '669']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(distinct_items)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_level_1_dict_level_2_list_of_size_3_tuples_to_list_of_list(data):\n",
    "    return [map(lambda y: y[0], data[x]) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.12297408e-01,  -3.59947309e-02,   4.36375029e-02,\n",
       "        -6.58282712e-02,  -5.41042387e-02,  -2.02573940e-01,\n",
       "         5.55509552e-02,  -7.79519603e-02,   3.09215277e-01,\n",
       "         2.49680951e-01,   3.67670804e-01,   3.21497858e-01,\n",
       "         9.67275128e-02,  -2.09786549e-01,  -5.31960139e-03,\n",
       "        -1.81605160e-01,  -1.28780171e-01,  -2.49864742e-01,\n",
       "        -1.77400455e-01,   1.84372351e-01,   1.68940768e-01,\n",
       "         2.54767090e-01,   8.53701606e-02,  -1.20013831e-02,\n",
       "         1.35253340e-01,   7.03064725e-02,   2.16806889e-01,\n",
       "        -2.46946931e-01,   1.01544999e-01,  -1.72019288e-01,\n",
       "        -1.32090813e-02,   4.33598049e-02,  -7.59228989e-02,\n",
       "        -3.28450859e-01,   6.33343682e-02,   1.94664538e-01,\n",
       "         8.46228376e-02,   2.02172220e-01,   2.35088378e-01,\n",
       "         2.34534308e-01,  -2.34135330e-01,   5.79590686e-02,\n",
       "         3.68431094e-04,   5.25200069e-02,  -5.04669659e-02,\n",
       "         5.31099504e-03,  -4.69869189e-02,  -2.35796690e-01,\n",
       "         4.84678410e-02,  -1.14225641e-01,  -2.15366498e-01,\n",
       "         1.95508674e-01,   2.07882211e-01,  -1.32640917e-02,\n",
       "        -2.98773646e-02,  -1.88740045e-02,   2.09378779e-01,\n",
       "        -2.18216538e-01,  -2.42468312e-01,   2.22723588e-01,\n",
       "         1.42251700e-01,   1.01911128e-02,  -1.64064970e-02,\n",
       "        -2.69584119e-01,   6.14985172e-03,  -1.31152850e-02,\n",
       "        -2.30130441e-02,  -2.66174287e-01,  -1.11659683e-01,\n",
       "        -1.20146289e-01,   2.46928230e-01,  -2.06637383e-01,\n",
       "        -4.59900238e-02,  -2.20765352e-01,  -5.58300838e-02,\n",
       "        -1.34434059e-01,  -6.97201714e-02,   6.21633716e-02,\n",
       "         1.66741297e-01,  -5.43507747e-03,  -2.93009788e-01,\n",
       "         1.97957143e-01,  -5.57391606e-02,  -1.90795094e-01,\n",
       "        -2.80050069e-01,  -5.45427091e-02,  -3.83489132e-01,\n",
       "        -1.52695984e-01,   3.19632627e-02,   1.73369601e-01,\n",
       "        -2.22963989e-01,   2.24064514e-01,  -1.85206711e-01,\n",
       "         8.23943615e-02,  -1.01023518e-01,  -2.58448929e-01,\n",
       "        -3.95437926e-02,  -2.21037101e-02,   4.54551317e-02,\n",
       "         1.29660591e-01], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[[x for x in m.wv.vocab][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-01 20:23:22,489 : INFO : collecting all words and their counts\n",
      "2017-04-01 20:23:22,490 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-01 20:23:22,498 : INFO : collected 1678 word types from a corpus of 30000 raw words and 5156 sentences\n",
      "2017-04-01 20:23:22,498 : INFO : Loading a fresh vocabulary\n",
      "2017-04-01 20:23:22,503 : INFO : min_count=1 retains 1678 unique words (100% of original 1678, drops 0)\n",
      "2017-04-01 20:23:22,503 : INFO : min_count=1 leaves 30000 word corpus (100% of original 30000, drops 0)\n",
      "2017-04-01 20:23:22,507 : INFO : deleting the raw counts dictionary of 1678 items\n",
      "2017-04-01 20:23:22,507 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2017-04-01 20:23:22,508 : INFO : downsampling leaves estimated 29029 word corpus (96.8% of prior 30000)\n",
      "2017-04-01 20:23:22,508 : INFO : estimated required memory for 1678 words and 100 dimensions: 2181400 bytes\n",
      "2017-04-01 20:23:22,512 : INFO : resetting layer weights\n",
      "2017-04-01 20:23:22,527 : INFO : training model with 3 workers on 1678 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=1\n",
      "2017-04-01 20:23:22,527 : INFO : expecting 5156 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-01 20:23:23,529 : INFO : PROGRESS: at 11.48% examples, 66440 words/s, in_qsize 4, out_qsize 4\n",
      "2017-04-01 20:23:24,529 : INFO : PROGRESS: at 22.56% examples, 65330 words/s, in_qsize 4, out_qsize 3\n",
      "2017-04-01 20:23:25,529 : INFO : PROGRESS: at 32.75% examples, 63332 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-01 20:23:26,529 : INFO : PROGRESS: at 42.87% examples, 62167 words/s, in_qsize 1, out_qsize 1\n",
      "2017-04-01 20:23:27,529 : INFO : PROGRESS: at 52.77% examples, 61222 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-01 20:23:28,529 : INFO : PROGRESS: at 63.17% examples, 61099 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-01 20:23:29,529 : INFO : PROGRESS: at 74.98% examples, 62171 words/s, in_qsize 2, out_qsize 0\n",
      "2017-04-01 20:23:30,529 : INFO : PROGRESS: at 85.40% examples, 61950 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-01 20:23:31,529 : INFO : PROGRESS: at 95.80% examples, 61760 words/s, in_qsize 1, out_qsize 4\n",
      "2017-04-01 20:23:31,914 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-01 20:23:31,915 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-01 20:23:31,916 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-01 20:23:31,917 : INFO : training on 600000 raw words (580532 effective words) took 9.4s, 61836 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "start modeling\n",
      "modeling finished\n"
     ]
    }
   ],
   "source": [
    "m = train_a_model(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_a_model(para):\n",
    "    data = para['data']\n",
    "    num_features = para['num_features']\n",
    "    min_count = para['min_count']\n",
    "    window = para['window']\n",
    "    para_iter = para['iter']\n",
    "    batch_words = para['batch_words']\n",
    "\n",
    "    #list_of_list = convert_2_level_dict_to_list_of_list(data)\n",
    "    list_of_list = convert_level_1_dict_level_2_list_of_size_3_tuples_to_list_of_list(data)\n",
    "    #print 'list_of_list:', list_of_list\n",
    "\n",
    "    print 'start training'\n",
    "    model = gensim.models.Word2Vec(list_of_list, size=num_features, min_count=min_count, window=window, sg=0, iter=para_iter, batch_words=batch_words)\n",
    "    print 'training finished'\n",
    "    print 'start modeling'\n",
    "    representation = [m[x] for x in m.wv.vocab]\n",
    "    print 'modeling finished'\n",
    "    return model, representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = generate_mimic_data_train_set_with_specified_M_and_R(1000, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '100',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '140',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '148',\n",
       " '149',\n",
       " '150',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '156',\n",
       " '157',\n",
       " '158',\n",
       " '159',\n",
       " '160',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '164',\n",
       " '165',\n",
       " '166',\n",
       " '167',\n",
       " '168',\n",
       " '169',\n",
       " '170',\n",
       " '171',\n",
       " '172',\n",
       " '173',\n",
       " '174',\n",
       " '175',\n",
       " '176',\n",
       " '177',\n",
       " '178',\n",
       " '179',\n",
       " '180',\n",
       " '181',\n",
       " '182',\n",
       " '183',\n",
       " '184',\n",
       " '185',\n",
       " '186',\n",
       " '187',\n",
       " '188',\n",
       " '189',\n",
       " '190',\n",
       " '191',\n",
       " '192',\n",
       " '193',\n",
       " '194',\n",
       " '195',\n",
       " '196',\n",
       " '197',\n",
       " '198',\n",
       " '199',\n",
       " '200',\n",
       " '201',\n",
       " '202',\n",
       " '203',\n",
       " '204',\n",
       " '205',\n",
       " '206',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '210',\n",
       " '211',\n",
       " '212',\n",
       " '213',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '217',\n",
       " '218',\n",
       " '219',\n",
       " '220',\n",
       " '221',\n",
       " '222',\n",
       " '223',\n",
       " '224',\n",
       " '225',\n",
       " '226',\n",
       " '227',\n",
       " '228',\n",
       " '229',\n",
       " '230',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '235',\n",
       " '236',\n",
       " '237',\n",
       " '238',\n",
       " '239',\n",
       " '240',\n",
       " '241',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '247',\n",
       " '248',\n",
       " '249',\n",
       " '250',\n",
       " '251',\n",
       " '252',\n",
       " '253',\n",
       " '254',\n",
       " '255',\n",
       " '256',\n",
       " '257',\n",
       " '258',\n",
       " '259',\n",
       " '260',\n",
       " '261',\n",
       " '262',\n",
       " '263',\n",
       " '264',\n",
       " '265',\n",
       " '266',\n",
       " '267',\n",
       " '268',\n",
       " '269',\n",
       " '270',\n",
       " '271',\n",
       " '272',\n",
       " '273',\n",
       " '274',\n",
       " '275',\n",
       " '276',\n",
       " '277',\n",
       " '278',\n",
       " '279',\n",
       " '280',\n",
       " '281',\n",
       " '282',\n",
       " '283',\n",
       " '284',\n",
       " '285',\n",
       " '286',\n",
       " '287',\n",
       " '288',\n",
       " '289',\n",
       " '290',\n",
       " '291',\n",
       " '292',\n",
       " '293',\n",
       " '294',\n",
       " '295',\n",
       " '296',\n",
       " '297',\n",
       " '298',\n",
       " '299',\n",
       " '300',\n",
       " '301',\n",
       " '302',\n",
       " '303',\n",
       " '304',\n",
       " '305',\n",
       " '306',\n",
       " '307',\n",
       " '308',\n",
       " '309',\n",
       " '310',\n",
       " '311',\n",
       " '312',\n",
       " '313',\n",
       " '314',\n",
       " '315',\n",
       " '316',\n",
       " '317',\n",
       " '318',\n",
       " '319',\n",
       " '320',\n",
       " '321',\n",
       " '322',\n",
       " '323',\n",
       " '324',\n",
       " '325',\n",
       " '326',\n",
       " '327',\n",
       " '328',\n",
       " '329',\n",
       " '330',\n",
       " '331',\n",
       " '332',\n",
       " '333',\n",
       " '334',\n",
       " '335',\n",
       " '336',\n",
       " '337',\n",
       " '338',\n",
       " '339',\n",
       " '340',\n",
       " '341',\n",
       " '342',\n",
       " '343',\n",
       " '344',\n",
       " '345',\n",
       " '346',\n",
       " '347',\n",
       " '348',\n",
       " '349',\n",
       " '350',\n",
       " '351',\n",
       " '352',\n",
       " '353',\n",
       " '354',\n",
       " '355',\n",
       " '356',\n",
       " '357',\n",
       " '358',\n",
       " '359',\n",
       " '360',\n",
       " '361',\n",
       " '362',\n",
       " '363',\n",
       " '364',\n",
       " '365',\n",
       " '366',\n",
       " '367',\n",
       " '368',\n",
       " '369',\n",
       " '370',\n",
       " '371',\n",
       " '372',\n",
       " '373',\n",
       " '374',\n",
       " '375',\n",
       " '376',\n",
       " '377',\n",
       " '378',\n",
       " '379',\n",
       " '380',\n",
       " '381',\n",
       " '382',\n",
       " '383',\n",
       " '384',\n",
       " '385',\n",
       " '386',\n",
       " '387',\n",
       " '388',\n",
       " '389',\n",
       " '390',\n",
       " '391',\n",
       " '392',\n",
       " '393',\n",
       " '394',\n",
       " '395',\n",
       " '396',\n",
       " '397',\n",
       " '398',\n",
       " '399',\n",
       " '400',\n",
       " '401',\n",
       " '402',\n",
       " '403',\n",
       " '404',\n",
       " '405',\n",
       " '406',\n",
       " '407',\n",
       " '408',\n",
       " '409',\n",
       " '410',\n",
       " '411',\n",
       " '412',\n",
       " '413',\n",
       " '414',\n",
       " '415',\n",
       " '416',\n",
       " '417',\n",
       " '418',\n",
       " '419',\n",
       " '420',\n",
       " '421',\n",
       " '422',\n",
       " '423',\n",
       " '424',\n",
       " '425',\n",
       " '426',\n",
       " '427',\n",
       " '428',\n",
       " '429',\n",
       " '430',\n",
       " '431',\n",
       " '432',\n",
       " '433',\n",
       " '434',\n",
       " '435',\n",
       " '436',\n",
       " '437',\n",
       " '438',\n",
       " '439',\n",
       " '440',\n",
       " '441',\n",
       " '442',\n",
       " '443',\n",
       " '444',\n",
       " '445',\n",
       " '446',\n",
       " '447',\n",
       " '448',\n",
       " '449',\n",
       " '450',\n",
       " '451',\n",
       " '452',\n",
       " '453',\n",
       " '454',\n",
       " '455',\n",
       " '456',\n",
       " '457',\n",
       " '458',\n",
       " '459',\n",
       " '460',\n",
       " '461',\n",
       " '462',\n",
       " '463',\n",
       " '464',\n",
       " '465',\n",
       " '466',\n",
       " '467',\n",
       " '468',\n",
       " '469',\n",
       " '470',\n",
       " '471',\n",
       " '472',\n",
       " '473',\n",
       " '474',\n",
       " '475',\n",
       " '476',\n",
       " '477',\n",
       " '478',\n",
       " '479',\n",
       " '480',\n",
       " '481',\n",
       " '482',\n",
       " '483',\n",
       " '484',\n",
       " '485',\n",
       " '486',\n",
       " '487',\n",
       " '488',\n",
       " '489',\n",
       " '490',\n",
       " '491',\n",
       " '492',\n",
       " '493',\n",
       " '494',\n",
       " '495',\n",
       " '496',\n",
       " '497',\n",
       " '498',\n",
       " '499',\n",
       " '500',\n",
       " '501',\n",
       " '502',\n",
       " '503',\n",
       " '504',\n",
       " '505',\n",
       " '506',\n",
       " '507',\n",
       " '508',\n",
       " '509',\n",
       " '510',\n",
       " '511',\n",
       " '512',\n",
       " '513',\n",
       " '514',\n",
       " '515',\n",
       " '516',\n",
       " '517',\n",
       " '518',\n",
       " '519',\n",
       " '520',\n",
       " '521',\n",
       " '522',\n",
       " '523',\n",
       " '524',\n",
       " '525',\n",
       " '526',\n",
       " '527',\n",
       " '528',\n",
       " '529',\n",
       " '530',\n",
       " '531',\n",
       " '532',\n",
       " '533',\n",
       " '534',\n",
       " '535',\n",
       " '536',\n",
       " '537',\n",
       " '538',\n",
       " '539',\n",
       " '540',\n",
       " '541',\n",
       " '542',\n",
       " '543',\n",
       " '544',\n",
       " '545',\n",
       " '546',\n",
       " '547',\n",
       " '548',\n",
       " '549',\n",
       " '550',\n",
       " '551',\n",
       " '552',\n",
       " '553',\n",
       " '554',\n",
       " '555',\n",
       " '556',\n",
       " '557',\n",
       " '558',\n",
       " '559',\n",
       " '560',\n",
       " '561',\n",
       " '562',\n",
       " '563',\n",
       " '564',\n",
       " '565',\n",
       " '566',\n",
       " '567',\n",
       " '568',\n",
       " '569',\n",
       " '570',\n",
       " '571',\n",
       " '572',\n",
       " '573',\n",
       " '574',\n",
       " '575',\n",
       " '576',\n",
       " '577',\n",
       " '578',\n",
       " '579',\n",
       " '580',\n",
       " '581',\n",
       " '582',\n",
       " '583',\n",
       " '584',\n",
       " '585',\n",
       " '586',\n",
       " '587',\n",
       " '588',\n",
       " '589',\n",
       " '590',\n",
       " '591',\n",
       " '592',\n",
       " '593',\n",
       " '594',\n",
       " '595',\n",
       " '596',\n",
       " '597',\n",
       " '598',\n",
       " '599',\n",
       " '600',\n",
       " '601',\n",
       " '602',\n",
       " '603',\n",
       " '604',\n",
       " '605',\n",
       " '606',\n",
       " '607',\n",
       " '608',\n",
       " '609',\n",
       " '610',\n",
       " '611',\n",
       " '612',\n",
       " '613',\n",
       " '614',\n",
       " '615',\n",
       " '616',\n",
       " '617',\n",
       " '618',\n",
       " '619',\n",
       " '620',\n",
       " '621',\n",
       " '622',\n",
       " '623',\n",
       " '624',\n",
       " '625',\n",
       " '626',\n",
       " '627',\n",
       " '628',\n",
       " '629',\n",
       " '630',\n",
       " '631',\n",
       " '632',\n",
       " '633',\n",
       " '634',\n",
       " '635',\n",
       " '636',\n",
       " '637',\n",
       " '638',\n",
       " '639',\n",
       " '640',\n",
       " '641',\n",
       " '642',\n",
       " '643',\n",
       " '644',\n",
       " '645',\n",
       " '646',\n",
       " '647',\n",
       " '648',\n",
       " '649',\n",
       " '650',\n",
       " '651',\n",
       " '652',\n",
       " '653',\n",
       " '654',\n",
       " '655',\n",
       " '656',\n",
       " '657',\n",
       " '658',\n",
       " '659',\n",
       " '660',\n",
       " '661',\n",
       " '662',\n",
       " '663',\n",
       " '664',\n",
       " '665',\n",
       " '666',\n",
       " '667',\n",
       " '668',\n",
       " '669',\n",
       " '670',\n",
       " '671',\n",
       " '672',\n",
       " '673',\n",
       " '674',\n",
       " '675',\n",
       " '676',\n",
       " '677',\n",
       " '678',\n",
       " '679',\n",
       " '680',\n",
       " '681',\n",
       " '682',\n",
       " '683',\n",
       " '684',\n",
       " '685',\n",
       " '686',\n",
       " '687',\n",
       " '688',\n",
       " '689',\n",
       " '690',\n",
       " '691',\n",
       " '692',\n",
       " '693',\n",
       " '694',\n",
       " '695',\n",
       " '696',\n",
       " '697',\n",
       " '698',\n",
       " '699',\n",
       " '700',\n",
       " '701',\n",
       " '702',\n",
       " '703',\n",
       " '704',\n",
       " '705',\n",
       " '706',\n",
       " '707',\n",
       " '708',\n",
       " '709',\n",
       " '710',\n",
       " '711',\n",
       " '712',\n",
       " '713',\n",
       " '714',\n",
       " '715',\n",
       " '716',\n",
       " '717',\n",
       " '718',\n",
       " '719',\n",
       " '720',\n",
       " '721',\n",
       " '722',\n",
       " '723',\n",
       " '724',\n",
       " '725',\n",
       " '726',\n",
       " '727',\n",
       " '728',\n",
       " '729',\n",
       " '730',\n",
       " '731',\n",
       " '732',\n",
       " '733',\n",
       " '734',\n",
       " '735',\n",
       " '736',\n",
       " '737',\n",
       " '738',\n",
       " '739',\n",
       " '740',\n",
       " '741',\n",
       " '742',\n",
       " '743',\n",
       " '744',\n",
       " '745',\n",
       " '746',\n",
       " '747',\n",
       " '748',\n",
       " '749',\n",
       " '750',\n",
       " '751',\n",
       " '752',\n",
       " '753',\n",
       " '754',\n",
       " '755',\n",
       " '756',\n",
       " '757',\n",
       " '758',\n",
       " '759',\n",
       " '760',\n",
       " '761',\n",
       " '762',\n",
       " '763',\n",
       " '764',\n",
       " '765',\n",
       " '766',\n",
       " '767',\n",
       " '768',\n",
       " '769',\n",
       " '770',\n",
       " '771',\n",
       " '772',\n",
       " '773',\n",
       " '774',\n",
       " '775',\n",
       " '776',\n",
       " '777',\n",
       " '778',\n",
       " '779',\n",
       " '780',\n",
       " '781',\n",
       " '782',\n",
       " '783',\n",
       " '784',\n",
       " '785',\n",
       " '786',\n",
       " '787',\n",
       " '788',\n",
       " '789',\n",
       " '790',\n",
       " '791',\n",
       " '792',\n",
       " '793',\n",
       " '794',\n",
       " '795',\n",
       " '796',\n",
       " '797',\n",
       " '798',\n",
       " '799',\n",
       " '800',\n",
       " '801',\n",
       " '802',\n",
       " '803',\n",
       " '804',\n",
       " '805',\n",
       " '806',\n",
       " '807',\n",
       " '808',\n",
       " '809',\n",
       " '810',\n",
       " '811',\n",
       " '812',\n",
       " '813',\n",
       " '814',\n",
       " '815',\n",
       " '816',\n",
       " '817',\n",
       " '818',\n",
       " '819',\n",
       " '820',\n",
       " '821',\n",
       " '822',\n",
       " '823',\n",
       " '824',\n",
       " '825',\n",
       " '826',\n",
       " '827',\n",
       " '828',\n",
       " '829',\n",
       " '830',\n",
       " '831',\n",
       " '832',\n",
       " '833',\n",
       " '834',\n",
       " '835',\n",
       " '836',\n",
       " '837',\n",
       " '838',\n",
       " '839',\n",
       " '840',\n",
       " '841',\n",
       " '842',\n",
       " '843',\n",
       " '844',\n",
       " '845',\n",
       " '846',\n",
       " '847',\n",
       " '848',\n",
       " '849',\n",
       " '850',\n",
       " '851',\n",
       " '852',\n",
       " '853',\n",
       " '854',\n",
       " '855',\n",
       " '856',\n",
       " '857',\n",
       " '858',\n",
       " '859',\n",
       " '860',\n",
       " '861',\n",
       " '862',\n",
       " '863',\n",
       " '864',\n",
       " '865',\n",
       " '866',\n",
       " '867',\n",
       " '868',\n",
       " '869',\n",
       " '870',\n",
       " '871',\n",
       " '872',\n",
       " '873',\n",
       " '874',\n",
       " '875',\n",
       " '876',\n",
       " '877',\n",
       " '878',\n",
       " '879',\n",
       " '880',\n",
       " '881',\n",
       " '882',\n",
       " '883',\n",
       " '884',\n",
       " '885',\n",
       " '886',\n",
       " '887',\n",
       " '888',\n",
       " '889',\n",
       " '890',\n",
       " '891',\n",
       " '892',\n",
       " '893',\n",
       " '894',\n",
       " '895',\n",
       " '896',\n",
       " '897',\n",
       " '898',\n",
       " '899',\n",
       " '900',\n",
       " '901',\n",
       " '902',\n",
       " '903',\n",
       " '904',\n",
       " '905',\n",
       " '906',\n",
       " '907',\n",
       " '908',\n",
       " '909',\n",
       " '910',\n",
       " '911',\n",
       " '912',\n",
       " '913',\n",
       " '914',\n",
       " '915',\n",
       " '916',\n",
       " '917',\n",
       " '918',\n",
       " '919',\n",
       " '920',\n",
       " '921',\n",
       " '922',\n",
       " '923',\n",
       " '924',\n",
       " '925',\n",
       " '926',\n",
       " '927',\n",
       " '928',\n",
       " '929',\n",
       " '930',\n",
       " '931',\n",
       " '932',\n",
       " '933',\n",
       " '934',\n",
       " '935',\n",
       " '936',\n",
       " '937',\n",
       " '938',\n",
       " '939',\n",
       " '940',\n",
       " '941',\n",
       " '942',\n",
       " '943',\n",
       " '944',\n",
       " '945',\n",
       " '946',\n",
       " '947',\n",
       " '948',\n",
       " '949',\n",
       " '950',\n",
       " '951',\n",
       " '952',\n",
       " '953',\n",
       " '954',\n",
       " '955',\n",
       " '956',\n",
       " '957',\n",
       " '958',\n",
       " '959',\n",
       " '960',\n",
       " '961',\n",
       " '962',\n",
       " '963',\n",
       " '964',\n",
       " '965',\n",
       " '966',\n",
       " '967',\n",
       " '968',\n",
       " '969',\n",
       " '970',\n",
       " '971',\n",
       " '972',\n",
       " '973',\n",
       " '974',\n",
       " '975',\n",
       " '976',\n",
       " '977',\n",
       " '978',\n",
       " '979',\n",
       " '980',\n",
       " '981',\n",
       " '982',\n",
       " '983',\n",
       " '984',\n",
       " '985',\n",
       " '986',\n",
       " '987',\n",
       " '988',\n",
       " '989',\n",
       " '990',\n",
       " '991',\n",
       " '992',\n",
       " '993',\n",
       " '994',\n",
       " '995',\n",
       " '996',\n",
       " '997',\n",
       " '998',\n",
       " '999']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_mimic_data_train_set_with_specified_M_and_R(num_of_movies, num_of_rates):\n",
    "    num_of_users = num_of_rates / num_of_movies\n",
    "    train = {}\n",
    "    history = [str(x) for x in range(num_of_movies)]\n",
    "    for x in xrange(num_of_users):\n",
    "        train[x] = history[:]\n",
    "        \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-01 20:51:14,666 : INFO : collecting all words and their counts\n",
      "2017-04-01 20:51:14,667 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-01 20:51:14,727 : INFO : collected 10 word types from a corpus of 1000000 raw words and 1000 sentences\n",
      "2017-04-01 20:51:14,727 : INFO : Loading a fresh vocabulary\n",
      "2017-04-01 20:51:14,728 : INFO : min_count=1 retains 10 unique words (100% of original 10, drops 0)\n",
      "2017-04-01 20:51:14,731 : INFO : min_count=1 leaves 1000000 word corpus (100% of original 1000000, drops 0)\n",
      "2017-04-01 20:51:14,732 : INFO : deleting the raw counts dictionary of 10 items\n",
      "2017-04-01 20:51:14,732 : INFO : sample=0.001 downsamples 9 most-common words\n",
      "2017-04-01 20:51:14,733 : INFO : downsampling leaves estimated 104820 word corpus (10.5% of prior 1000000)\n",
      "2017-04-01 20:51:14,733 : INFO : estimated required memory for 10 words and 100 dimensions: 13000 bytes\n",
      "2017-04-01 20:51:14,734 : INFO : resetting layer weights\n",
      "2017-04-01 20:51:14,734 : INFO : training model with 3 workers on 10 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=1\n",
      "2017-04-01 20:51:14,735 : INFO : expecting 1000 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preparing training set.\n",
      "preparing training set done.\n",
      "len(train): 1000\n",
      "currently, num_of_movies=1000, num_of_rates=1000000.\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-01 20:51:15,736 : INFO : PROGRESS: at 21.37% examples, 447911 words/s, in_qsize 4, out_qsize 5\n",
      "2017-04-01 20:51:16,736 : INFO : PROGRESS: at 45.38% examples, 475612 words/s, in_qsize 5, out_qsize 4\n",
      "2017-04-01 20:51:17,737 : INFO : PROGRESS: at 64.99% examples, 454185 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-01 20:51:18,737 : INFO : PROGRESS: at 86.53% examples, 453504 words/s, in_qsize 4, out_qsize 2\n",
      "2017-04-01 20:51:19,429 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-01 20:51:19,430 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-01 20:51:19,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-01 20:51:19,431 : INFO : training on 20000000 raw words (2096260 effective words) took 4.7s, 446401 effective words/s\n",
      "2017-04-01 20:51:19,555 : INFO : collecting all words and their counts\n",
      "2017-04-01 20:51:19,556 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-01 20:51:19,619 : INFO : collected 10 word types from a corpus of 999000 raw words and 333 sentences\n",
      "2017-04-01 20:51:19,620 : INFO : Loading a fresh vocabulary\n",
      "2017-04-01 20:51:19,621 : INFO : min_count=1 retains 10 unique words (100% of original 10, drops 0)\n",
      "2017-04-01 20:51:19,621 : INFO : min_count=1 leaves 999000 word corpus (100% of original 999000, drops 0)\n",
      "2017-04-01 20:51:19,621 : INFO : deleting the raw counts dictionary of 10 items\n",
      "2017-04-01 20:51:19,622 : INFO : sample=0.001 downsamples 9 most-common words\n",
      "2017-04-01 20:51:19,622 : INFO : downsampling leaves estimated 90310 word corpus (9.0% of prior 999000)\n",
      "2017-04-01 20:51:19,623 : INFO : estimated required memory for 10 words and 100 dimensions: 13000 bytes\n",
      "2017-04-01 20:51:19,623 : INFO : resetting layer weights\n",
      "2017-04-01 20:51:19,624 : INFO : training model with 3 workers on 10 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=1\n",
      "2017-04-01 20:51:19,624 : INFO : expecting 333 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "start modeling\n",
      "modeling finished\n",
      "interval: 4.88822\n",
      "start preparing training set.\n",
      "preparing training set done.\n",
      "len(train): 333\n",
      "currently, num_of_movies=3000, num_of_rates=1000000.\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-01 20:51:20,627 : INFO : PROGRESS: at 23.54% examples, 426251 words/s, in_qsize 5, out_qsize 5\n",
      "2017-04-01 20:51:21,627 : INFO : PROGRESS: at 46.98% examples, 424813 words/s, in_qsize 4, out_qsize 3\n",
      "2017-04-01 20:51:22,628 : INFO : PROGRESS: at 70.47% examples, 424503 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-01 20:51:23,629 : INFO : PROGRESS: at 94.29% examples, 425852 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-01 20:51:23,875 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-01 20:51:23,876 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-01 20:51:23,877 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-01 20:51:23,883 : INFO : training on 19980000 raw words (1806652 effective words) took 4.3s, 424358 effective words/s\n",
      "2017-04-01 20:51:24,004 : INFO : collecting all words and their counts\n",
      "2017-04-01 20:51:24,005 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-01 20:51:24,069 : INFO : collected 10 word types from a corpus of 1000000 raw words and 200 sentences\n",
      "2017-04-01 20:51:24,070 : INFO : Loading a fresh vocabulary\n",
      "2017-04-01 20:51:24,070 : INFO : min_count=1 retains 10 unique words (100% of original 10, drops 0)\n",
      "2017-04-01 20:51:24,072 : INFO : min_count=1 leaves 1000000 word corpus (100% of original 1000000, drops 0)\n",
      "2017-04-01 20:51:24,072 : INFO : deleting the raw counts dictionary of 10 items\n",
      "2017-04-01 20:51:24,073 : INFO : sample=0.001 downsamples 9 most-common words\n",
      "2017-04-01 20:51:24,073 : INFO : downsampling leaves estimated 92383 word corpus (9.2% of prior 1000000)\n",
      "2017-04-01 20:51:24,073 : INFO : estimated required memory for 10 words and 100 dimensions: 13000 bytes\n",
      "2017-04-01 20:51:24,074 : INFO : resetting layer weights\n",
      "2017-04-01 20:51:24,075 : INFO : training model with 3 workers on 10 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=1\n",
      "2017-04-01 20:51:24,075 : INFO : expecting 200 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "start modeling\n",
      "modeling finished\n",
      "interval: 4.44458\n",
      "start preparing training set.\n",
      "preparing training set done.\n",
      "len(train): 200\n",
      "currently, num_of_movies=5000, num_of_rates=1000000.\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-01 20:51:25,078 : INFO : PROGRESS: at 24.93% examples, 460701 words/s, in_qsize 4, out_qsize 3\n",
      "2017-04-01 20:51:26,079 : INFO : PROGRESS: at 49.35% examples, 456322 words/s, in_qsize 4, out_qsize 3\n",
      "2017-04-01 20:51:27,080 : INFO : PROGRESS: at 74.12% examples, 456811 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-01 20:51:28,079 : INFO : PROGRESS: at 98.72% examples, 456338 words/s, in_qsize 6, out_qsize 3\n",
      "2017-04-01 20:51:28,121 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-01 20:51:28,122 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-01 20:51:28,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-01 20:51:28,124 : INFO : training on 20000000 raw words (1849474 effective words) took 4.0s, 456900 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "start modeling\n",
      "modeling finished\n",
      "interval: 4.23725\n",
      "[(1000, 1000000, 4.88821816444397), (3000, 1000000, 4.444576025009155), (5000, 1000000, 4.237254858016968)]\n"
     ]
    }
   ],
   "source": [
    "#train_set_size_list = 10 ** np.array(range(1, 4 + 1))\n",
    "#train_set_size_list = 10**4 * np.array(range(1, 5 + 1))\n",
    "#iter_list = [20, 30, 40]\n",
    "\n",
    "#num_of_movies = None #1000\n",
    "#num_of_rates = 10 * 10000\n",
    "\n",
    "num_of_movies_list = [1000, 3000, 5000]\n",
    "num_of_rates_list = 10**6 * np.array(range(1, 3 + 1))\n",
    "\n",
    "metric = []\n",
    "\n",
    "###\n",
    "\n",
    "test_data_inner_ratio = 0.5  # insignificant in this notebook\n",
    "s, mc, w = 100, 1, 1         # insignificant in this notebook\n",
    "batch_words = 1          # insignificant in this notebook\n",
    "para_iter = 20\n",
    "\n",
    "seed = 2 \n",
    "\n",
    "for __i, num_of_movies in enumerate(num_of_movies_list):\n",
    "    for __j, num_of_rates in enumerate(num_of_rates_list):\n",
    "        random.seed(seed)\n",
    "        \n",
    "        print 'start preparing training set.'\n",
    "        #train = extract_data_from_file_and_generate_train_set_with_specified_M_and_R(data_filename, num_of_movies, num_of_rates, delimiter)\n",
    "        train = generate_mimic_data_train_set_with_specified_M_and_R(num_of_movies, num_of_rates)\n",
    "        print 'preparing training set done.'\n",
    "        print 'len(train):', len(train)\n",
    "\n",
    "        para = {'data': train, \n",
    "            'model_name': 'main_model',\n",
    "            'num_features': s,\n",
    "            'min_count': mc,\n",
    "            'window': w,\n",
    "            'iter': para_iter,\n",
    "            'batch_words': batch_words,\n",
    "        }\n",
    "        ##\n",
    "        print \"currently, num_of_movies=%d, num_of_rates=%d.\" % (num_of_movies, num_of_rates)\n",
    "        \n",
    "        #starttime = datetime.datetime.now()             # start timing\n",
    "        starttime = time.time()\n",
    "        train_a_model(para)\n",
    "\n",
    "        #endtime = datetime.datetime.now()               # stop timing\n",
    "        endtime = time.time()               # stop timing\n",
    "        \n",
    "        #interval = (endtime - starttime).seconds\n",
    "        interval = endtime - starttime\n",
    "        print 'interval: %g' % (interval)\n",
    "        \n",
    "        metric.append((num_of_movies, num_of_rates, interval))\n",
    "        \n",
    "        break\n",
    "    #break\n",
    "print metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100000, 0.4424738883972168)\n",
      "(3000, 100000, 0.4133009910583496)\n",
      "(5000, 100000, 0.40193986892700195)\n"
     ]
    }
   ],
   "source": [
    "for x in metric:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
